import mechanicalsoup as ms  
import redis
from elasticsearch import Elasticsearch, helpers
import configparser

config = configparser.ConfigParser()
config.read('example.ini')

es = Elasticsearch(
    cloud_id=config['ELASTIC']['cloud_id'],
    http_auth=(config['ELASTIC']['user'], config['ELASTIC']['password'])
)

def write2elastic(es, url, html):
    es.index(
    index="webpages",
    document={
    'url': url.decode('utf-8'),
    'html': html
    })

def crawl(es, r, browser, url):
    print("downloading url")
    browser.open(url)
    # parse for links

    write2elastic(es, url, str(browser.page))

    atags = browser.page.find_all("a")
    hrefs = [a.get("href") for a in atags]

    # add links to redis queue
    domain = "https://en.wikipedia.org"
    links = [(domain + href) for href in hrefs if href and href.startswith("/wiki/")]
    r.lpush("links", *links)

r = redis.Redis()
#root url  
browser = ms.StatefulBrowser()
# download webpage
start_url = "https://en.wikipedia.org/wiki/BMW"
r.lpush("links", start_url)

while url := r.rpop("links"):

    crawl(es, r, browser, url)
    print("crawling " + str(url))