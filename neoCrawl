import mechanicalsoup as ms  
import redis
from elasticsearch import Elasticsearch, helpers
import configparser
from neo4j import GraphDatabase

class Neo4JConnector:
    def __init__(self, uri, user, password) -> None:
       self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()
    
    def add_links(self, page, links):
        with self.driver.session() as session:
            session.execute_write(self._create_links, page, links)
    
    @staticmethod
    def _create_links(tx, page, links):
        for link in links:
            tx.run("CREATE (:Page {url: $link}) -[:LINKS_TO]-> (:Page {url: $page} )", 
                            page=page, link=str(link))

neo4J_connector = Neo4JConnector("bolt://localhost:7689", "neo4j", "12345678")

config = configparser.ConfigParser()
config.read('example.ini')

es = Elasticsearch(
    cloud_id=config['ELASTIC']['cloud_id'],
    http_auth=(config['ELASTIC']['user'], config['ELASTIC']['password'])
)

def write2elastic(es, url, html):
    es.index(
    index="webpages",
    document={
    'url': url.decode('utf-8'),
    'html': html
    })

def crawl(es, r, browser, neo4J_connector, link):
    print("downloading url")
    browser.open(url)
    # parse for links

    write2elastic(es, url, str(browser.page))

    atags = browser.page.find_all("a")
    hrefs = [a.get("href") for a in atags]

    # add links to redis queue
    domain = "https://en.wikipedia.org"
    links = [(domain + href) for href in hrefs if href and href.startswith("/wiki/")]
    r.lpush("links", *links)
    neo4J_connector.add_links(url, links)

r = redis.Redis()
#root url  
browser = ms.StatefulBrowser()
# download webpage
start_url = "https://en.wikipedia.org/wiki/BMW"
r.lpush("links", start_url)

while url := r.rpop("links"):

    crawl(es, r, browser, neo4J_connector, url)
    print("crawling " + str(url))

neo4J_connector.close()